{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    "# This code is part of Qiskit.\n",
    "#\n",
    "# (C) Copyright IBM 2019.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "#\n",
    "# Code adapted from QizGloria team, Qiskit Camp Europe 2019, updated by \n",
    "# Team Ube Pancake, Qiskit Summer Jam 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumRegister,QuantumCircuit,ClassicalRegister,execute\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 1\n",
    "\n",
    "NUM_SHOTS = 10000\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to translate Q-Circuit parameters from pytorch back to QISKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numbers(tensor_list):\n",
    "    num_list = []\n",
    "    for tensor in tensor_list:\n",
    "        num_list += [tensor.item()]\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QiskitCircuit()\n",
    "\n",
    "Parameters have to be defined according to the circuit. In this case we have a unitary U3 with 3 parameters, which we call Theta, Phi and Lambda.\n",
    "The paramter `shots` always has to be defined. It gives the number of measurements done on the simulator or on the hardware to do the averages for the expectation values.\n",
    "\n",
    "### create_circuit()\n",
    "Define a quantum cirquit in QISKIT language\n",
    "\n",
    "### N_qubit_expectation_Z()\n",
    "This is a function that allows to take the measurements and translate them to Z-expectation values for every single qubit.\n",
    "\n",
    "### bind( parameters )\n",
    "Takes the Neural Network parameters and puts them into a format that can be fed into the QIKSIT unitaries.\n",
    "\n",
    "To generalze this code we would have to write a function that replaces the line `self.circuit.data[2][0]._params`.\n",
    "Because the `self.circuit.data` is a list of all the gates  of the circuit. And the indices direct to the correct gate, where the parameters have to go.\n",
    "\n",
    "At some point we would need a better scheme here.\n",
    "\n",
    "### run( parameters )\n",
    "This function puts the circuit with the given parameters on a quantum simulater or hardware and returns the measurements of every qubit in Z-basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T16:09:30.598730Z",
     "start_time": "2019-10-01T16:09:30.567861Z"
    }
   },
   "outputs": [],
   "source": [
    "class QiskitCircuit():\n",
    "    \n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        # --- Circuit definition ---\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        \n",
    "        self.theta = Parameter('Theta')\n",
    "        self.phi = Parameter('Phi')\n",
    "        self.lam = Parameter('Lambda')\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        self.circuit.u3(self.theta, self.phi, self.lam, all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        self.circuit.measure_all()\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n",
    "        expects = np.zeros(nr_qubits)\n",
    "        for key in counts.keys():\n",
    "            perc = counts[key]/shots\n",
    "            check = np.array([(float(key[i])-1/2)*2*perc for i in range(nr_qubits)])\n",
    "            expects += check   \n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        job_sim = execute(self.circuit,\n",
    "                          self.backend,\n",
    "                          shots=self.shots,\n",
    "                          parameter_binds=[{self.theta : i[0].item(),\n",
    "                                            self.phi : i[1].item(),\n",
    "                                            self.lam : i[2].item()}])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        return self.N_qubit_expectation_Z(counts,self.shots,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e7de2f08ae71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcircuit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQiskitCircuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIMULATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SHOTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print('Expected value for rotation [pi/4, pi/4, pi/4]: {}'.format(circuit.run(torch.Tensor([np.pi/4, np.pi/4, np.pi/4]))[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected value for rotation [pi/4]: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mpl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Figures/{}-qubit u3-circuit.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_QUBITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-9254f4f04b18>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     34\u001b[0m                           \u001b[0mshots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                           parameter_binds=[{self.theta : i[0].item(),\n\u001b[0;32m---> 36\u001b[0;31m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                                             self.lam : i[2].item()}])\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "circuit = QiskitCircuit(1, SIMULATOR, NUM_SHOTS)\n",
    "#print('Expected value for rotation [pi/4, pi/4, pi/4]: {}'.format(circuit.run(torch.Tensor([np.pi/4, np.pi/4, np.pi/4]))[0]))\n",
    "print('Expected value for rotation [pi/4]: {}'.format(circuit.run(torch.Tensor([np.pi/4]))[0]))\n",
    "circuit.circuit.draw(output='mpl', filename='Figures/{}-qubit u3-circuit.jpg'.format(NUM_QUBITS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchCircuit()\n",
    "\n",
    "A pytorch layer always has two functions. One for the forward pass and one for the backward pass. The forward pass simply takes the Quantum Circuits variational parameters from the previous pytorch layer and runs the circuit on the defined hardware (defined in `QiskitCircuit.run()`) and returns the measurements from the quantum hardware.\n",
    "These measurements will be the inputs of the next pytorch layer.\n",
    "\n",
    "The backward pass returns the gradients of the quantum circuit. In this case here it is finite difference.\n",
    "\n",
    "the `forward_tensor` is saved from the forward pass. So we just have to do one evaluation of the Q-Circuit in the backpass for the finite difference.\n",
    "\n",
    "The `gradient` variable here is as well hard coded to 3 parameters. This should be updated in the future and made more general.\n",
    "\n",
    "The loop `for k in range(len(input_numbers)):` goes through all the parameters (in this case 3), and shifts them by a small $\\epsilon$. Then it runs the circuit and takes the diefferences of the ouput for the parameters $\\Theta$ and $\\Theta + \\epsilon$. This is the finite difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):    \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QiskitCirc'):\n",
    "            ctx.QiskitCirc = QiskitCircuit(1, SIMULATOR, shots=NUM_SHOTS)\n",
    "            \n",
    "        exp_value = ctx.QiskitCirc.run(i[0])\n",
    "        \n",
    "        result = torch.tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = 0.01\n",
    "        \n",
    "        forward_tensor, i = ctx.saved_tensors\n",
    "#         print(i)\n",
    "        input_numbers = to_numbers(i[0])\n",
    "#         print(input_numbers)\n",
    "        gradient = [0,0,0]\n",
    "        \n",
    "        for k in range(len(input_numbers)):\n",
    "            input_eps = input_numbers\n",
    "            input_eps[k] = input_numbers[k] + eps\n",
    "            exp_value = ctx.QiskitCirc.run(torch.tensor(input_eps))[0]\n",
    "            gradient_result = (exp_value - forward_tensor[0][0].item())#/eps\n",
    "            gradient[k] = gradient_result\n",
    "            \n",
    "#         print(gradient)\n",
    "        result = torch.tensor([gradient])\n",
    "#         print(result)\n",
    "\n",
    "        return result.float() * grad_output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0212, 0.0296, 0.0186]])\n"
     ]
    }
   ],
   "source": [
    "#x = torch.tensor([[np.pi/4, np.pi/4, np.pi/4]], requires_grad=True)\n",
    "x = torch.tensor([[np.pi/4]], requires_grad=True)\n",
    "# x = torch.tensor([[0.0, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "qc = TorchCircuit.apply\n",
    "y1 = qc(x)\n",
    "y1.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Quantum Circuit separately\n",
    "\n",
    "This example is simply to test the QC with a pytorch optimizer\n",
    "\n",
    "We define a cost function and a target expectation value (here -1). The cost is the square distance from the target value.\n",
    "\n",
    "`x` is the initialization of the parameters. Here again, this was hard coded such that every angle starts at $\\pi / 4$.\n",
    "\n",
    "The rest is standard pytorch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  8.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd9eacd5310>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)\n",
    "    return torch.abs(qc(x) - target) ** 2, expval\n",
    "\n",
    "#x = torch.tensor([[-np.pi/2, -np.pi/2, -np.pi/2]], requires_grad=True)\n",
    "x = torch.tensor([[-np.pi/2]], requires_grad=True)\n",
    "opt = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "num_epoch = 50\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "# for i in range(num_epoch):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval.item())\n",
    "#     print(loss.item())\n",
    "\n",
    "plt.plot(loss_list)\n",
    "    \n",
    "# print(circuit(phi, theta))\n",
    "# print(cost(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST\n",
    "\n",
    "In this code we can not handle batches yet.\n",
    "This should be implemented as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor, save\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "#from torch.nn import Linear\n",
    "#from torch.nn import ReLU\n",
    "#from torch.nn import Softmax\n",
    "#from torch.nn import Module\n",
    "#from torch.optim import SGD\n",
    "#from torch.nn import CrossEntropyLoss\n",
    "\n",
    "img_dimensions = 300\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "n_datapoints = 100\n",
    "\n",
    "'''\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "labels = mnist_trainset.targets #get labels\n",
    "labels = labels.numpy()\n",
    "idx1 = np.where(labels == 0) #search all zeros\n",
    "idx2 = np.where(labels == 1) # search all ones\n",
    "idx = np.concatenate((idx1[0][0:n_datapoints//2],idx2[0][0:n_datapoints//2])) # concatenate their indices\n",
    "mnist_trainset.targets = labels[idx] \n",
    "mnist_trainset.data = mnist_trainset.data[idx]\n",
    "\n",
    "print(mnist_trainset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size_train, shuffle=True)\n",
    "'''\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_dimensions, img_dimensions)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "img_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_dimensions,img_dimensions)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "train_data_path = \"/home/wtf/dogs-vs-cats/train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "\n",
    "validation_data_path = \"/home/wtf/dogs-vs-cats/validation/\"\n",
    "validation_data = torchvision.datasets.ImageFolder(root=validation_data_path,transform=img_test_transforms, is_valid_file=check_image)\n",
    "\n",
    "test_data_path = \"/home/wtf/dogs-vs-cats/test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_test_transforms, is_valid_file=check_image)\n",
    "\n",
    "num_workers = 6\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validation_data_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network with Q-node\n",
    "\n",
    "This NN is  2 layers of ConvNN and a fully connected layer, with a Q-Node as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        #self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = np.pi*torch.tanh(x)\n",
    "        #x = qc(x) # This is the q node\n",
    "        x = (x+1)/2 # Translate expectation values [-1,1] to labels [0,1]\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "standard pytorch training loop.\n",
    "- Load data from train_loader. Which is this case a single example each step.\n",
    "- Forward pass through NN\n",
    "- Caluculate loss\n",
    "- Backprop and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1ef87ed80c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Calculating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/johnson/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-e025c93f212a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#x = x.view(-1, 320)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/johnson/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/johnson/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/johnson/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(batch_idx)\n",
    "        optimizer.zero_grad()        \n",
    "        # Forward pass\n",
    "        print(data.shape)\n",
    "        output = network(data)\n",
    "        # Calculating loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Neg Log Likelihood Loss')\n",
    "plt.savefig('Figures/{}-qubit Loss Curve u3.jpg'.format(NUM_QUBITS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of NN\n",
    "\n",
    "The outcome is not always the same because the prediction is probabilistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "number = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    number +=1\n",
    "    output = network(data)\n",
    "    output = (output>0.5).float()\n",
    "    accuracy += (output[0][1].item() == target[0].item())*1\n",
    "    \n",
    "print(\"Performance on test data is is: {}\".format(accuracy/number))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = network(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
